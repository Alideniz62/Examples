{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2bdeb-1987-4f08-89ac-8d844af16a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "val stockTicksDf = spark.readStream\n",
    "                    .format(\"kafka\")\n",
    "                    .option(\"kafka.bootstrap.servers\",\"hadoop-vm:9092\")\n",
    "                    .option(\"subscribe\", \"stock-ticks\")\n",
    "                    .option(\"group-id\", \"stock-ticks-groupAk-hdfs\")\n",
    "                    .option(\"header\", true)\n",
    "                    .option(\"inferSchema\", true)\n",
    "                    .option(\"delimitter\", \",\")\n",
    "                    .load()\n",
    "stockTicksDf.printSchema(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b0182-b560-43dd-b007-86ad283de255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val ticksDf = stockTicksDf.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
    "ticksDf.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed506e2e-9aa6-4180-bc0a-5c8af76c8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StringType, StructType, DoubleType, \n",
    "                                   IntegerType, LongType, StructField }\n",
    "\n",
    "val stockTicksschema = StructType( List(\n",
    "    StructField(\"symbol\", StringType, true),\n",
    "    StructField(\"price\", DoubleType, true),\n",
    "    StructField(\"volume\", LongType, true),\n",
    "    StructField(\"timestamp\", LongType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ff6fe-abc1-4436-961c-2913b5117c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val jsonDf = ticksDf.withColumn(\"value\", from_json($\"value\", stockTicksschema))\n",
    "jsonDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7b509-c0e7-4b94-b04d-52a8e4dd4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val stockTickDf = jsonDf.select (col(\"value.*\")) \n",
    "stockTickDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72620e0-5a48-4517-a582-79a31abe7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val stockTickDf1  = stockTickDf \n",
    "                .withColumn(\"timestampTemp\", (col(\"timestamp\") / 1000).cast(\"timestamp\"))\n",
    "                .withColumn(\"trade_time\", date_trunc(\"minute\", col(\"timestampTemp\")))\n",
    "                .drop(\"timestamp\")\n",
    "                .drop(\"timestampTemp\")\n",
    "                .withColumnRenamed(\"trade_time\", \"timestamp\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1ccbb-2031-4d2e-b4c2-e9f1ec70a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "val stockTickDf1Min = stockTickDf\\\n",
    "                            .selectExpr(\"timestamp\", \"symbol\",  \"struct(*) as value\")\\\n",
    "                            .withWatermark(\"timestamp\", \"1 minutes\")\\\n",
    "                            .groupBy(\"symbol\", F.window(\"timestamp\", \"60 seconds\"))\\\n",
    "                            .agg( F.collect_list(\"value\").alias(\"ticks\"))\n",
    "\n",
    "stockTickDf1Min.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef1b46-6c7e-46d3-b6fd-400bd4ad6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// import org.apache.spark.sql.functions.{date_format}\n",
    "// val stockTickDf2 = stockTickDf1.withColumn(\"Year\", date_format($\"timestamp\", \"yyyy\"))\n",
    "//                      .withColumn(\"Month\", date_format($\"timestamp\", \"MM\"))\n",
    "//                      .withColumn(\"Day\", date_format($\"timestamp\", \"dd\"))\n",
    "//                      .withColumn(\"Hours\",date_format($\"timestamp\",\"hh\"))\n",
    "//                      .withColumn(\"Minutes\",date_format($\"timestamp\",\"mm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a853f2-2b66-439c-8401-a2e589d0ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "def processBatchData(candleBatchDf:DataFrame, batch_id:Long)={\n",
    "    print (\"process batch called\", batch_id, \"writing \", candleBatchDf.count())\n",
    "   \n",
    "import org.apache.spark.sql.functions.{explode,col}\n",
    " \n",
    "var candleTickBatchDf =  (candleBatchDf\n",
    "        .coalesce(1)\n",
    "        .write\n",
    "        .partitionBy(\"Year\", \"Month\", \"Day\" , \"Hours\", \"Minutes\", \"symbol\")\n",
    "        .mode(\"append\")\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", true)\n",
    "         .save(\"hdfs://localhost:9000/stock-ticks-scala12\")\n",
    "\n",
    "    )\n",
    "}\n",
    "       \n",
    "stockTickDf2.writeStream.\n",
    "foreachBatch(processBatchData _).outputMode(\"append\").start()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04152961-947a-4363-bcf9-4a96d9d5fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "//import pyspark.sql.functions as F\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "\n",
    "    stockTickDf2\n",
    "     .withColumn(\"year\", date_format(col(\"timestamp\"), \"yyyy\"))\n",
    "     .withColumn(\"month\", date_format(col(\"timestamp\"), \"MM\"))\n",
    "     .withColumn(\"day\", date_format(col(\"timestamp\"), \"dd\"))  \n",
    "     .withColumn(\"hour\", date_format(col(\"timestamp\"), \"HH\"))   \n",
    "     .withColumn(\"_symbol\", col(\"symbol\"))   \n",
    "    .writeStream\n",
    "    .trigger(Trigger.ProcessingTime(\"2 seconds\"))\n",
    "    .queryName(\"Write Ticks to CSV trigger by 1 min hour\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"hdfs://localhost:9000/dump-csv-trigger-hourly-1min/scala\")\n",
    "    .option(\"header\", true)\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:9000/checkpoint/tickscsvtohdfs6/scala\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\", \"hour\", \"_symbol\")\n",
    "    .option(\"truncate\", false)\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad8b5a-f305-4411-81b4-90546731aaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
